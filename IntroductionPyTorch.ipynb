{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "A tensor is a fundamental data structure for neural network. A tensor is generalization of matrix. A 1-dimensional tensor is vector, a two-dimensional tensor is matrix and n-dimensional array is tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "%config IPCompleter.greedy=True #by default the intellisense is not shown. with this line of code, it will\n",
    "\n",
    "#import PyTorch\n",
    "#Some tips for modules and installation\n",
    "#If you encounter module not found, then go to Anaconda Navigator-> Environments->base(root)->Open iphyton\n",
    "#In the terminal, run command pip install <packagename>\n",
    "!wget -c https://raw.githubusercontent.com/udacity/deep-learning-v2-pytorch/master/intro-to-pytorch/helper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function\n",
    "        Arguements\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def test_network(net, trainloader):\n",
    "\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "    dataiter = iter(trainloader)\n",
    "    images, labels = dataiter.next()\n",
    "\n",
    "    # Create Variables for the inputs and targets\n",
    "    inputs = Variable(images)\n",
    "    targets = Variable(images)\n",
    "\n",
    "    # Clear the gradients from all Variables\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass, then backward pass, then update weights\n",
    "    output = net.forward(inputs)\n",
    "    loss = criterion(output, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def imshow(image, ax=None, title=None, normalize=True):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    image = image.numpy().transpose((1, 2, 0))\n",
    "\n",
    "    if normalize:\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        image = std * image + mean\n",
    "        image = np.clip(image, 0, 1)\n",
    "\n",
    "    ax.imshow(image)\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.tick_params(axis='both', length=0)\n",
    "    ax.set_xticklabels('')\n",
    "    ax.set_yticklabels('')\n",
    "\n",
    "    return ax\n",
    "\n",
    "\n",
    "def view_recon(img, recon):\n",
    "    ''' Function for displaying an image (as a PyTorch Tensor) and its\n",
    "        reconstruction also a PyTorch Tensor\n",
    "    '''\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, sharex=True, sharey=True)\n",
    "    axes[0].imshow(img.numpy().squeeze())\n",
    "    axes[1].imshow(recon.data.numpy().squeeze())\n",
    "    for ax in axes:\n",
    "        ax.axis('off')\n",
    "        ax.set_adjustable('box-forced')\n",
    "\n",
    "def view_classify(img, ps, version=\"MNIST\"):\n",
    "    ''' Function for viewing an image and it's predicted classes.\n",
    "    '''\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    if version == \"MNIST\":\n",
    "        ax2.set_yticklabels(np.arange(10))\n",
    "    elif version == \"Fashion\":\n",
    "        ax2.set_yticklabels(['T-shirt/top',\n",
    "                            'Trouser',\n",
    "                            'Pullover',\n",
    "                            'Dress',\n",
    "                            'Coat',\n",
    "                            'Sandal',\n",
    "                            'Shirt',\n",
    "                            'Sneaker',\n",
    "                            'Bag',\n",
    "                            'Ankle Boot'], size='small');\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "def show_next_image(data_set:torch.utils.data.DataLoader) -> tuple:\n",
    "    image, label = iter(data_set).next()\n",
    "    imshow(image[0,:])\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7) #set the random seed so thigns are predictable\n",
    "\n",
    "features = torch.randn(1, 5) #random nomral variables with five elements\n",
    "\n",
    "weights = torch.randn_like(features) #true weights for our data. Same shape as features\n",
    "\n",
    "bias = torch.randn(1,1) #A single value for bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n",
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "# Calculate the output for this neural network with input features features, weights, and bias. We will use matrix multiplication\n",
    "# between features and weights and add bias element to it. The output is then passed on to activiation fucntion to generate the final result\n",
    "\n",
    "y = activation(torch.sum(features * weights)+bias) #here we do elements by element multiplicatio and bias\n",
    "print(y)\n",
    "\n",
    "# alternatively we could use sum function on tensor\n",
    "y = activation((features*weights).sum() +bias)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use matrix multiplication using torch.mm() for high performance\n",
    "# We are goign to resize the weights based on teh shape of features. For matrix multiplifcation we want 1 x 5 * 5 x 1. \n",
    "# we use view on tensor and specify desired shape to get a new tensor with same data element. The view takes (row, column)\n",
    "# this is simply saying activation(torch.mm(features, weigths.view(5,1))+bias). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "y = activation(torch.mm(features, weights.view(features.size()[1], features.size()[0])) +bias)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Network\n",
    "We have features and weights.The multiplicaiot of weigth and features makes a hidden units. \n",
    "\n",
    "             O           #One output unit \n",
    "\n",
    "         h1      h2      #Two hidden units\n",
    "         \n",
    "    x1       x2      x3  #Three inpu features\n",
    " \n",
    "Our problem is expressed as below:\n",
    "y = xi*wi + b\n",
    "\n",
    "                                                                 [w11 w12]\n",
    "                                                                 [w11 w12]\n",
    "     The hidden lay h is calcualed as h=[h1,h2] = [x1,x2,x3..xn].[:    : ]\n",
    "                                                                 [:    : ]\n",
    "                                                                 [wn1 wn2]\n",
    "                                                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "tensor([[0.5349, 0.1988, 0.6592]])\n",
      "Weights of features:\n",
      "tensor([[0.6569, 0.2328],\n",
      "        [0.4251, 0.2071],\n",
      "        [0.6297, 0.3653]])\n",
      "Weights of hidden units:\n",
      "tensor([[ 0.3775],\n",
      "        [-0.9509]])\n",
      "Hidden Units:\n",
      "tensor([[0.2959, 0.2123]])\n",
      "Output:\n",
      "tensor([[0.1409]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### Generate some data\n",
    "#set the random seed so we are getting the same set of data each time\n",
    "torch.manual_seed(7)\n",
    "\n",
    "#Features are three random normal variables\n",
    "features = torch.rand((1,3))\n",
    "\n",
    "#Define the size of each layer in the network\n",
    "n_input = features.shape[1] #Number of input units, this must match the number of input features. our input features is 1 x 3 matrix and thus we have 3 features in this case\n",
    "n_hidden = 2 #Number of hidden units\n",
    "n_output = 1 #Number of outputs\n",
    "\n",
    "\n",
    "#weigths for input to hidden layer\n",
    "w1 = torch.rand(n_input, n_hidden)\n",
    "\n",
    "#Weights for hidden layer to output layer\n",
    "w2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "#Bias\n",
    "B1 = torch.randn((1,n_hidden))\n",
    "B1 = torch.randn((1,n_output))\n",
    "\n",
    "\n",
    "#printing input data\n",
    "print(\"Features:\")\n",
    "print(features)\n",
    "\n",
    "print(\"Weights of features:\")\n",
    "print(w1)\n",
    "\n",
    "print(\"Weights of hidden units:\")\n",
    "print(w2)\n",
    "\n",
    "h = activation(torch.mm(features, w1)+B1)\n",
    "output = activation(torch.mm(h, w2)+B1)\n",
    "\n",
    "\n",
    "print(\"Hidden Units:\")\n",
    "print(h)\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy to Torch and back\n",
    "Numpy array and Torch are reversible. Memory is shared. Tensor has datatype information while numpy array does not have that info within the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07630829 0.77991879 0.43840923]\n",
      " [0.72346518 0.97798951 0.53849587]\n",
      " [0.50112046 0.07205113 0.26843898]\n",
      " [0.4998825  0.67923    0.80373904]]\n",
      "tensor([[0.0763, 0.7799, 0.4384],\n",
      "        [0.7235, 0.9780, 0.5385],\n",
      "        [0.5011, 0.0721, 0.2684],\n",
      "        [0.4999, 0.6792, 0.8037]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np               #import numpy\n",
    "np.random.seed(7)\n",
    "a = np.random.rand(4,3)          #a numpy array\n",
    "print(a)\n",
    "b = torch.from_numpy(a)          #creata a tensor from numpy array\\\n",
    "print(b)                         #note than tensor always have dtype which ndarray does not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15261658 1.55983758 0.87681846]\n",
      " [1.44693036 1.95597902 1.07699174]\n",
      " [1.00224093 0.14410227 0.53687796]\n",
      " [0.999765   1.35845999 1.60747807]]\n",
      "tensor([[0.1526, 1.5598, 0.8768],\n",
      "        [1.4469, 1.9560, 1.0770],\n",
      "        [1.0022, 0.1441, 0.5369],\n",
      "        [0.9998, 1.3585, 1.6075]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "b.numpy()                        #gives back numpy array. Memory is shared between numpy and torch\n",
    "b.mul_(2)                        #inplace operation of multipying by 2 on tensor, changes the value of numpy array\n",
    "print(a)                        #Notice the changed values\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks in PyTorch\n",
    "Deep implies network consisteing of massive layers. PyTorch's nn module simplifies the building of network consisteing of matrcies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary pacakges\n",
    "\n",
    "#\n",
    "# Explain what is %matplotlib and %config InlineBackend do\n",
    "#\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Use MNIST dataset which consists of greyscale hand written digits. \n",
    "#Each image is 28x28 pixels. Our goal is to build a neural network \n",
    "#that can take one of these images and predict the digit in the image.\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#define a transform to normalize the data\n",
    "transform = transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "#Download the training data\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform = transform)\n",
    "\n",
    "#Everytime we are getting images, we are getting a batch of 64 images.\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data loaded into trainloader and we can use iterator and loop through the data.\n",
    "Using iter is same as using for loop as below:\n",
    "for image, label in trainloader:\n",
    "    print(image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "trainloader\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2dac1e8b0c8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the batch of images images to build a multi-layer network.\n",
    "We have 64 images. Each image is of dimension 1 (channel) x 28 (height) x 28 (width). Thus one layer is really (64, 1, 28, 28). When this is flattened, we have 64 x 784 as we flattened teh 2D image to 1D vector.\n",
    "\n",
    "We have 256 hidden units, and 10 output units using random tensors for the weights and biases. We want 10 output to get probability for each of the numbers 0,1,2..9. \n",
    "\n",
    "In the network, we want to pass an image and get out a probability distribution over the classes that tells the likelihood of image belonging to those classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -4.0037, -10.5920, -15.2532,   1.6932,  18.8478,  13.8026,  -3.8765,\n",
      "           7.3375, -20.3124,   8.8694],\n",
      "        [ -3.7280,  -2.2973, -17.0857,   1.9139,  15.2778,   1.5556,   1.2317,\n",
      "          10.5055, -21.7185,  -0.2940],\n",
      "        [ -4.4372,  -4.7429, -16.9171,  -0.9745,  16.7698,  12.0000,   2.1957,\n",
      "          -0.5961, -18.2544,  -4.6790],\n",
      "        [ -1.9408,  -2.6723, -12.7742,   0.2631,  17.1021,  13.0295,  15.8355,\n",
      "           8.0205, -21.1886,   0.5963],\n",
      "        [ -6.8978, -10.9951,  -8.9972,   1.5503,  13.9366,   9.8109,  -2.4642,\n",
      "          -2.8042, -19.7631,  -8.9857],\n",
      "        [-11.1233,  -3.1031, -17.2671,  -1.8600,  19.3763,  14.6103,   7.9895,\n",
      "          -3.6405, -20.4525,  -7.7252],\n",
      "        [  9.2281,  -9.0607, -13.2973,   1.6014,  25.8930,   9.6311,  12.4725,\n",
      "          15.1879, -15.0954,  -4.9090],\n",
      "        [  3.3918,  -4.1994, -17.2525,  -0.3851,  16.1968,   6.7080,  -2.1579,\n",
      "           9.5091, -20.8402, -10.2223],\n",
      "        [  3.0596,  -8.4707, -21.2103,   4.5876,  17.3814,  14.8889,   2.2607,\n",
      "           1.8147, -21.4321,  -4.6967],\n",
      "        [  2.0335,   9.2317, -17.0880,  -2.0401,  25.4413,  16.4462,  11.6473,\n",
      "          20.6376, -21.5266,  -3.5433],\n",
      "        [ -3.8842, -13.5513, -13.8469,   0.5118,  23.2814,  13.6582,  -1.3529,\n",
      "           3.7091, -24.2348,   7.3393],\n",
      "        [ -1.7305,   2.9968, -19.4770,  -7.8653,  31.6378,  21.7299,  11.9237,\n",
      "           3.6942, -12.5669,  -1.3517],\n",
      "        [  2.3974,  -7.1011, -12.7711,  -7.8832,  13.1548,  17.0546,   5.2817,\n",
      "           9.5499, -25.7547,   5.2247],\n",
      "        [ -2.5672,   2.2764, -11.3256,  -0.5479,  17.1368,  10.4246,   9.9008,\n",
      "          12.9846, -21.6954,  -5.6804],\n",
      "        [  0.2244,  -2.4212, -19.5035,   0.1999,  16.3689,  15.1462,   1.2889,\n",
      "           3.0703, -19.2868,  -2.6874],\n",
      "        [  0.8161,  -3.0013, -21.9826,  10.3182,  18.6070,  -0.7448,   3.8633,\n",
      "           4.8066, -18.7319,  -9.9229],\n",
      "        [  2.0208,  -5.8864, -18.1823,  -5.2083,  17.7423,  10.2006,   7.8768,\n",
      "           8.0984, -13.4106,   4.5840],\n",
      "        [  1.4160,  -1.5553, -23.0358,  -8.3003,  23.4802,   5.7834,   2.1709,\n",
      "           7.0751, -22.4044,  -2.9811],\n",
      "        [-10.0790,   1.6426, -12.5070,   0.3114,  12.3418,   8.8428,   2.4679,\n",
      "           7.8919, -21.6450,  -6.3832],\n",
      "        [ -4.2888, -12.4295, -21.3677,  -0.6243,  15.4219,  10.9291,   8.6182,\n",
      "          -1.6631, -25.0213,  -1.8123],\n",
      "        [-11.3375, -12.5906,  -9.4487,  11.8407,  18.1491,  13.0241,   6.0658,\n",
      "           7.7114, -25.5873,  -4.1955],\n",
      "        [-10.7956,   0.8145,  -9.0901,  -3.3301,  18.8395,   8.8581,  15.3543,\n",
      "           9.8062, -22.8268,  -9.8456],\n",
      "        [ -7.8345, -14.8004, -13.8563,  -3.5253,  20.3569,   9.1866,   8.6793,\n",
      "           6.7483, -19.1173,   1.8876],\n",
      "        [-13.5388, -13.3369,  -8.7602,  -2.9337,  18.6984,   6.3232,  -0.3047,\n",
      "           3.4241, -20.4923,   1.2771],\n",
      "        [ -7.4625,  -7.3901, -10.4222,   7.3317,  13.3554,   9.8232,  -6.4561,\n",
      "           6.4694, -22.1133,   4.5352],\n",
      "        [ -5.4526,  -5.0267, -13.0664,   4.4804,  18.8606,  11.8177,  -1.3603,\n",
      "           8.2142, -20.5343,  -4.3140],\n",
      "        [ -6.2011,  -1.2086, -12.8448,   6.8134,  15.7852,  -0.1731,  -3.5695,\n",
      "           9.1183, -29.6496,  -1.2292],\n",
      "        [  6.9253,  -8.6019, -22.4201,  -3.7008,  18.2626,  18.8601,  -8.4578,\n",
      "           6.4915, -19.4967,   5.2567],\n",
      "        [ -7.8235, -10.3388,  -4.6158,   5.4708,  12.6245,  13.3175,   1.5980,\n",
      "           5.5621, -19.4689,  -8.6061],\n",
      "        [-12.5042,  -5.1490, -17.2535,   4.5336,  19.8725,  13.5722,   5.6025,\n",
      "          -6.0988, -18.0478, -17.4777],\n",
      "        [  7.2037,   5.7881, -26.5849,  -8.5205,  24.8079,   9.1954,   6.4525,\n",
      "          18.0375, -17.7050,  -8.5062],\n",
      "        [  7.3226,  12.8917, -18.3578,  -3.6870,  28.9919,   4.1893,   4.6248,\n",
      "          12.5843, -22.7834,  -3.5164],\n",
      "        [ -4.3917,   0.5205, -19.6989,  -5.6132,  11.6065,  -1.2471,   1.0731,\n",
      "          10.3925, -25.3169, -13.7343],\n",
      "        [  2.3164,   7.4817, -14.9984, -10.0890,  13.8953,  14.1996,   9.2317,\n",
      "           3.1201, -28.6517,  -8.6992],\n",
      "        [ -1.2122,  -1.5122, -21.2349,   4.3623,  27.0030,  10.3717,   8.9937,\n",
      "           3.5807, -18.7702,  -5.6040],\n",
      "        [ -4.9216, -11.8495, -17.7666,   4.3297,   9.2792,   4.5042,  -7.1844,\n",
      "          -2.8268, -21.7996,   1.9602],\n",
      "        [ -8.9580,  -6.3289, -12.8728,  -4.3052,  23.2651,   1.2062,  -5.8082,\n",
      "           4.1706, -19.9009,  -6.1051],\n",
      "        [ -8.8776,   0.8676, -10.9114,   1.2703,  17.5084,   6.3323,   2.3140,\n",
      "           7.5474, -21.2674,  -8.2914],\n",
      "        [  3.6838,   1.7460,  -9.4052,   9.6844,  14.5543,  16.5468,   8.5186,\n",
      "          -0.7342, -22.9144,   1.7692],\n",
      "        [ -3.4847,  -6.6212, -13.9489,   0.8143,  23.3495,  10.6380,  11.1834,\n",
      "          10.9100, -26.2705,  -6.0591],\n",
      "        [ -2.4095,   6.3888, -25.7545, -12.3777,  20.6556,  13.1861,  11.4274,\n",
      "          12.8888, -21.4002,  -9.4073],\n",
      "        [  0.3981,   0.3892, -15.3936,  -4.1100,  26.5963,  -0.2976,   8.5706,\n",
      "           4.7183, -22.9918,  -4.8993],\n",
      "        [ -6.6651,   3.9003, -17.7286,  -4.8950,  25.7947,  -2.5270,  -8.9731,\n",
      "           7.9154, -25.5629, -13.7050],\n",
      "        [ -4.8703,  -8.1378, -16.1117,   3.0775,  20.6643,   4.0341,  11.6508,\n",
      "           6.9832, -16.4045,  -4.1163],\n",
      "        [  4.9406,  -7.4191, -17.6541,  -2.2515,  19.0644,   7.4091,   8.3701,\n",
      "          -5.1247, -21.5312,   1.9860],\n",
      "        [ -6.8392,   0.4240, -18.5373,  -0.5115,  17.9618,   3.1512,  -4.5904,\n",
      "           7.2493, -23.2755,  -7.2083],\n",
      "        [ -2.8367,   6.1282, -21.3144,  -0.6900,  30.7231,   5.7229,  12.4487,\n",
      "          12.2448, -28.9177,  -6.3026],\n",
      "        [ -6.9001,  -8.2316, -18.4243,   6.5984,  16.3069,  13.2460,   6.0854,\n",
      "           5.0787, -20.4559,  -3.9874],\n",
      "        [ -5.0139,   3.3545, -16.7557,   2.6320,  19.9643,   9.1840,   7.8950,\n",
      "           2.0317, -20.3340,  -7.8343],\n",
      "        [ -0.1890,  -6.7656, -20.4711,  -7.1184,  23.0711,  11.1273,   1.7262,\n",
      "          14.0491, -17.1596,  -2.1277],\n",
      "        [ -0.3330,   9.9703, -12.5918,  -8.6633,  18.3108,   3.5941,   3.7545,\n",
      "           8.0482, -25.7420,  -1.4530],\n",
      "        [ -8.5144,  -2.1153,  -5.3820,   7.3370,  15.8920,  12.4526,  -0.1208,\n",
      "           3.6471, -19.2852,  -7.0152],\n",
      "        [-16.6112,  -4.3963, -20.3983,  -1.1436,  29.7244,  13.8143,   3.7847,\n",
      "           7.6589, -22.7298,  -3.3202],\n",
      "        [ -7.6326,  -1.0569, -13.2909,   7.5401,  16.2943,  11.7511,   1.9088,\n",
      "           2.2300, -27.7055,  -1.3135],\n",
      "        [ -4.1242,  -8.1960,  -9.1999,   5.0739,  20.6059,  11.4781,   8.1578,\n",
      "           5.9736, -22.1302,  -2.0518],\n",
      "        [ -4.6021,  -4.8990, -15.6592,   5.9596,  14.7183,   7.7852,   0.3677,\n",
      "          -4.8313, -27.8939,  -5.2359],\n",
      "        [  3.2451,  10.4369,  -9.2111,  -3.3023,  19.0379,   5.3791,  16.3659,\n",
      "          11.8067, -30.7758,  -3.2715],\n",
      "        [-13.5363,  -0.6179, -13.2760,   4.1011,  16.9018,  -1.6289,  12.9838,\n",
      "           0.4335, -28.0498,  -3.2251],\n",
      "        [ -0.2049,   3.0377, -11.1938,   3.1280,  13.9234,   3.5928,   8.1860,\n",
      "           0.9772, -23.3660,  -5.4392],\n",
      "        [  9.6365,   5.8079, -24.5639,  -1.3177,  15.2183,  19.5801,   6.1622,\n",
      "          -2.3770, -26.4870,  -1.5137],\n",
      "        [-10.3239,  -1.6304, -12.4424,   0.7996,  16.6617,   2.8786,   9.9806,\n",
      "           4.6097, -19.1614,   1.1791],\n",
      "        [ -5.9848,  -4.8175, -17.2956,   3.2624,  16.9307,  17.3363,   4.5019,\n",
      "           5.8558, -25.4492,  -2.3105],\n",
      "        [-17.7645, -17.6517,  -5.3152,  -0.2797,  10.5530,   3.3556,  -1.8091,\n",
      "           6.7970, -26.3611,  -1.9140],\n",
      "        [ -1.8441,  -9.3382,  -9.1827,   6.6083,  17.7288,   8.1428,  -4.1928,\n",
      "          -5.4739, -19.9655,  -5.8096]])\n"
     ]
    }
   ],
   "source": [
    "#Activiation function was previously defined (sigmoid)\n",
    "\n",
    "#Flatten the input images\n",
    "inputs = images.view(images.shape[0], -1) #-1 is shortcut to match the best shape\n",
    "\n",
    "#Create parameters\n",
    "w1 = torch.randn(784, 256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "w2 = torch.randn(256, 10)\n",
    "b2 = torch.randn(10)\n",
    "\n",
    "h = activation(torch.mm(inputs, w1)+b1)\n",
    "out = torch.mm(h,w2)+b2\n",
    "\n",
    "print(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "What does softmax do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1,1)\n",
    "\n",
    "probabilities = softmax(out)\n",
    "print(probabilities.shape)\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch using nn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #input to hidden layer\n",
    "        self.hidden = nn.Linear(784,256)\n",
    "        \n",
    "        #hidden to output\n",
    "        self.output = nn.Linear(256,10)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a model using the class\n",
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternaive way to build network is using funcational module\n",
    "#fairly similar to pevious way\n",
    "#Normally imported as capital F.\n",
    "# A little succinct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #input to hidden layer\n",
    "        self.hidden = nn.Linear(784,256)\n",
    "        #hidden to output\n",
    "        self.output = nn.Linear(256,10)        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Hidden layer with sigmoid activation\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        #Output layer with Softmax actication\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (3): ReLU()\n",
       "  (4): Linear(in_features=64, out_features=10, bias=True)\n",
       "  (5): LogSoftmax()\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets crate a model with two hidden layer with ReLU, one input and one output\n",
    "\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784,128), \n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128,64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64,10),\n",
    "    nn.LogSoftmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3132, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "#Define a loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "#get data\n",
    "images, labels = next(iter(trainloader))\n",
    "\n",
    "#flatten images\n",
    "images = images.view(images.shape[0],-1)\n",
    "\n",
    "#forward pass, get out logits\n",
    "logps = model(images)\n",
    "\n",
    "#calculate the loss with the logits and the lables\n",
    "loss = criterion(logps, labels)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "tensor([0.], grad_fn=<PowBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Autograd automatically calcualtes gradients for tensors which is simple a switch for tensors\n",
    "x = torch.zeros(1, requires_grad=True)\n",
    "print(x)\n",
    "\n",
    "#globally it can be set with torch.set_grad_enabled(True | False)\n",
    "\n",
    "y = x**2\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " tensor([[-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "        [-0.0016, -0.0016, -0.0016,  ..., -0.0016, -0.0016, -0.0016],\n",
      "        [-0.0018, -0.0018, -0.0018,  ..., -0.0018, -0.0018, -0.0018],\n",
      "        ...,\n",
      "        [-0.0061, -0.0061, -0.0061,  ..., -0.0061, -0.0061, -0.0061],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0012,  0.0012,  0.0012,  ...,  0.0012,  0.0012,  0.0012]])\n",
      "After backward pass: \n",
      " tensor([[-0.0008, -0.0008, -0.0008,  ..., -0.0008, -0.0008, -0.0008],\n",
      "        [-0.0037, -0.0037, -0.0037,  ..., -0.0037, -0.0037, -0.0037],\n",
      "        [-0.0043, -0.0043, -0.0043,  ..., -0.0043, -0.0043, -0.0043],\n",
      "        ...,\n",
      "        [-0.0087, -0.0087, -0.0087,  ..., -0.0087, -0.0087, -0.0087],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0048,  0.0048,  0.0048,  ...,  0.0048,  0.0048,  0.0048]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "loss.backward()\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0177,  0.0150,  0.0161,  ...,  0.0046,  0.0185, -0.0028],\n",
      "        [-0.0138, -0.0072,  0.0218,  ...,  0.0114,  0.0220,  0.0060],\n",
      "        [ 0.0254, -0.0203,  0.0050,  ...,  0.0232, -0.0088, -0.0013],\n",
      "        ...,\n",
      "        [ 0.0112,  0.0326, -0.0263,  ..., -0.0066, -0.0066, -0.0119],\n",
      "        [-0.0239, -0.0044, -0.0199,  ..., -0.0016,  0.0225,  0.0034],\n",
      "        [-0.0238, -0.0019, -0.0098,  ..., -0.0143, -0.0083,  0.0322]],\n",
      "       requires_grad=True)\n",
      "Gradient - tensor([[-0.0007, -0.0007, -0.0007,  ..., -0.0007, -0.0007, -0.0007],\n",
      "        [ 0.0011,  0.0011,  0.0011,  ...,  0.0011,  0.0011,  0.0011],\n",
      "        [-0.0026, -0.0026, -0.0026,  ..., -0.0026, -0.0026, -0.0026],\n",
      "        ...,\n",
      "        [-0.0053, -0.0053, -0.0053,  ..., -0.0053, -0.0053, -0.0053],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0007,  0.0007,  0.0007,  ...,  0.0007,  0.0007,  0.0007]])\n",
      "Updated weights - Parameter containing:\n",
      "tensor([[ 0.0177,  0.0150,  0.0161,  ...,  0.0046,  0.0185, -0.0028],\n",
      "        [-0.0138, -0.0072,  0.0218,  ...,  0.0114,  0.0220,  0.0060],\n",
      "        [ 0.0254, -0.0203,  0.0050,  ...,  0.0232, -0.0088, -0.0013],\n",
      "        ...,\n",
      "        [ 0.0112,  0.0326, -0.0263,  ..., -0.0066, -0.0066, -0.0118],\n",
      "        [-0.0239, -0.0044, -0.0199,  ..., -0.0016,  0.0225,  0.0034],\n",
      "        [-0.0238, -0.0019, -0.0098,  ..., -0.0143, -0.0083,  0.0322]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#Optimizer is used to update teh weights with the gradients. This is available in optim package\n",
    "\n",
    "from torch import optim\n",
    "\n",
    "#Optimizer requires the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "\n",
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64,784)\n",
    "\n",
    "optimizer.zero_grad()\n",
    "\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "\n",
    "print('Gradient -', model[0].weight.grad)\n",
    "\n",
    "optimizer.step()\n",
    "print('Updated weights -', model[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 0.5218960524542626\n",
      "Training loss: 0.23290917393304647\n",
      "Training loss: 0.16780525905244958\n",
      "Training loss: 0.13337222628617115\n",
      "Training loss: 0.11093939403826589\n",
      "Training loss: 0.09442869992529564\n",
      "Training loss: 0.08385049485202346\n",
      "Training loss: 0.07471964332505442\n",
      "Training loss: 0.0663720972794813\n",
      "Training loss: 0.06007774040373022\n",
      "Training loss: 0.05360310056310361\n",
      "Training loss: 0.04883436949328899\n",
      "Training loss: 0.04461260826109962\n",
      "Training loss: 0.04171621272714485\n",
      "Training loss: 0.03784318605251958\n",
      "Last loss was 0.2635434596205571\n"
     ]
    }
   ],
   "source": [
    "#Final Model\n",
    "\n",
    "\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(784,128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128,64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64,10),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optimizer = optim.SGD(model.parameters(), lr=0.05)\n",
    "epoch = 5\n",
    "\n",
    "for e in range(epoch):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        images = images.view(images.shape[0],-1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(images)\n",
    "        \n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")\n",
    "\n",
    "print('Last loss was 0.2635434596205571')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 235,
       "width": 424
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Lets do prediction now\n",
    "\n",
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "img = images[0].view(1,784)\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "ps = F.softmax(logits, dim=1)\n",
    "view_classify(img.view(1, 28,28), ps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtain fasion mnist dataset\n",
    "\n",
    "#Use MNIST dataset which consists of greyscale hand written digits. \n",
    "#Each image is 28x28 pixels. Our goal is to build a neural network \n",
    "#that can take one of these images and predict the digit in the image.\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#define a transform to normalize the data\n",
    "transform_fashion = transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "training_fashion = datasets.FashionMNIST(root = \"./data\", train = True, download = True, transform = transform_fashion, target_transform=None)\n",
    "#Everytime we are getting images, we are getting a batch of 64 images.\n",
    "train_batch_fashion = torch.utils.data.DataLoader(training_fashion, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fashion mnist dataset\n",
    "\n",
    "#Network architecture\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.log_softmax(self.fc4(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 1.5152292185183018\n",
      "Training loss: 1.8625791829023788\n",
      "Training loss: 1.9242506353839883\n",
      "Training loss: 1.855768910857406\n",
      "Training loss: 1.8508604894568925\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Classifier()    #Define model\n",
    "criterion = nn.NLLLoss()    #Define criterion\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.05)    #Define criterion a different one than SDG\n",
    "\n",
    "epoch = 5\n",
    "\n",
    "\n",
    "\n",
    "for e in range(epoch):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_batch_fashion:\n",
    "        images[1]\n",
    "        logps = model(images)    #reshaping is happening in the classifier. This is short cut to passing a model and do forward pass\n",
    "        loss = criterion(logps, labels)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()     \n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataitem' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-150-2477d10e3303>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdataiter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_batch_fashion\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataitem' is not defined"
     ]
    }
   ],
   "source": [
    "#Predict\n",
    "\n",
    "dataiter = iter(train_batch_fashion)\n",
    "images, labels = dataitem.next()\n",
    "img = images[1]\n",
    "\n",
    "ps = torch.exp(model(img))\n",
    "\n",
    "view_classify(img, ps, version='Fashion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
