{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors\n",
    "A tensor is a fundamental data structure for neural network. A tensor is generalization of matrix. A 1-dimensional tensor is vector, a two-dimensional tensor is matrix and n-dimensional array is tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True #by default the intellisense is not shown. with this line of code, it will\n",
    "\n",
    "#import PyTorch\n",
    "#Some tips for modules and installation\n",
    "#If you encounter module not found, then go to Anaconda Navigator-> Environments->base(root)->Open iphyton\n",
    "#In the terminal, run command pip install <packagename>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function\n",
    "        Arguements\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7) #set the random seed so thigns are predictable\n",
    "\n",
    "features = torch.randn(1, 5) #random nomral variables with five elements\n",
    "\n",
    "weights = torch.randn_like(features) #true weights for our data. Same shape as features\n",
    "\n",
    "bias = torch.randn(1,1) #A single value for bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n",
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "# Calculate the output for this neural network with input features features, weights, and bias. We will use matrix multiplication\n",
    "# between features and weights and add bias element to it. The output is then passed on to activiation fucntion to generate the final result\n",
    "\n",
    "y = activation(torch.sum(features * weights)+bias) #here we do elements by element multiplicatio and bias\n",
    "print(y)\n",
    "\n",
    "# alternatively we could use sum function on tensor\n",
    "y = activation((features*weights).sum() +bias)\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use matrix multiplication using torch.mm() for high performance\n",
    "# We are goign to resize the weights based on teh shape of features. For matrix multiplifcation we want 1 x 5 * 5 x 1. \n",
    "# we use view on tensor and specify desired shape to get a new tensor with same data element. The view takes (row, column)\n",
    "# this is simply saying activation(torch.mm(features, weigths.view(5,1))+bias). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "y = activation(torch.mm(features, weights.view(features.size()[1], features.size()[0])) +bias)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multilayer Network\n",
    "We have features and weights.The multiplicaiot of weigth and features makes a hidden units. \n",
    "\n",
    "             O           #One output unit \n",
    "\n",
    "         h1      h2      #Two hidden units\n",
    "         \n",
    "    x1       x2      x3  #Three inpu features\n",
    " \n",
    "Our problem is expressed as below:\n",
    "y = xi*wi + b\n",
    "\n",
    "                                                                 [w11 w12]\n",
    "                                                                 [w11 w12]\n",
    "     The hidden lay h is calcualed as h=[h1,h2] = [x1,x2,x3..xn].[:    : ]\n",
    "                                                                 [:    : ]\n",
    "                                                                 [wn1 wn2]\n",
    "                                                                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      "tensor([[0.5349, 0.1988, 0.6592]])\n",
      "Weights of features:\n",
      "tensor([[0.6569, 0.2328],\n",
      "        [0.4251, 0.2071],\n",
      "        [0.6297, 0.3653]])\n",
      "Weights of hidden units:\n",
      "tensor([[ 0.3775],\n",
      "        [-0.9509]])\n",
      "Hidden Units:\n",
      "tensor([[0.2959, 0.2123]])\n",
      "Output:\n",
      "tensor([[0.1409]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "### Generate some data\n",
    "#set the random seed so we are getting the same set of data each time\n",
    "torch.manual_seed(7)\n",
    "\n",
    "#Features are three random normal variables\n",
    "features = torch.rand((1,3))\n",
    "\n",
    "#Define the size of each layer in the network\n",
    "n_input = features.shape[1] #Number of input units, this must match the number of input features. our input features is 1 x 3 matrix and thus we have 3 features in this case\n",
    "n_hidden = 2 #Number of hidden units\n",
    "n_output = 1 #Number of outputs\n",
    "\n",
    "\n",
    "#weigths for input to hidden layer\n",
    "w1 = torch.rand(n_input, n_hidden)\n",
    "\n",
    "#Weights for hidden layer to output layer\n",
    "w2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "#Bias\n",
    "B1 = torch.randn((1,n_hidden))\n",
    "B1 = torch.randn((1,n_output))\n",
    "\n",
    "\n",
    "#printing input data\n",
    "print(\"Features:\")\n",
    "print(features)\n",
    "\n",
    "print(\"Weights of features:\")\n",
    "print(w1)\n",
    "\n",
    "print(\"Weights of hidden units:\")\n",
    "print(w2)\n",
    "\n",
    "h = activation(torch.mm(features, w1)+B1)\n",
    "output = activation(torch.mm(h, w2)+B1)\n",
    "\n",
    "\n",
    "print(\"Hidden Units:\")\n",
    "print(h)\n",
    "print(\"Output:\")\n",
    "print(output)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy to Torch and back\n",
    "Numpy array and Torch are reversible. Memory is shared. Tensor has datatype information while numpy array does not have that info within the data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.07630829 0.77991879 0.43840923]\n",
      " [0.72346518 0.97798951 0.53849587]\n",
      " [0.50112046 0.07205113 0.26843898]\n",
      " [0.4998825  0.67923    0.80373904]]\n",
      "tensor([[0.0763, 0.7799, 0.4384],\n",
      "        [0.7235, 0.9780, 0.5385],\n",
      "        [0.5011, 0.0721, 0.2684],\n",
      "        [0.4999, 0.6792, 0.8037]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np               #import numpy\n",
    "np.random.seed(7)\n",
    "a = np.random.rand(4,3)          #a numpy array\n",
    "print(a)\n",
    "b = torch.from_numpy(a)          #creata a tensor from numpy array\\\n",
    "print(b)                         #note than tensor always have dtype which ndarray does not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.15261658 1.55983758 0.87681846]\n",
      " [1.44693036 1.95597902 1.07699174]\n",
      " [1.00224093 0.14410227 0.53687796]\n",
      " [0.999765   1.35845999 1.60747807]]\n",
      "tensor([[0.1526, 1.5598, 0.8768],\n",
      "        [1.4469, 1.9560, 1.0770],\n",
      "        [1.0022, 0.1441, 0.5369],\n",
      "        [0.9998, 1.3585, 1.6075]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "b.numpy()                        #gives back numpy array. Memory is shared between numpy and torch\n",
    "b.mul_(2)                        #inplace operation of multipying by 2 on tensor, changes the value of numpy array\n",
    "print(a)                        #Notice the changed values\n",
    "print(b)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks in PyTorch\n",
    "Deep implies network consisteing of massive layers. PyTorch's nn module simplifies the building of network consisteing of matrcies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessary pacakges\n",
    "\n",
    "#\n",
    "# Explain what is %matplotlib and %config InlineBackend do\n",
    "#\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import helper\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Use MNIST dataset which consists of greyscale hand written digits. \n",
    "#Each image is 28x28 pixels. Our goal is to build a neural network \n",
    "#that can take one of these images and predict the digit in the image.\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "#define a transform to normalize the data\n",
    "transform = transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.5], [0.5])])\n",
    "\n",
    "#Download the training data\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform = transform)\n",
    "\n",
    "#Everytime we are getting images, we are getting a batch of 64 images.\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the training data loaded into trainloader and we can use iterator and loop through the data.\n",
    "Using iter is same as using for loop as below:\n",
    "for image, label in trainloader:\n",
    "    print(image.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "trainloader\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = dataiter.next()\n",
    "print(type(images))\n",
    "print(images.shape)\n",
    "print(labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x2dac1e8b0c8>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 248,
       "width": 251
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(images[1].numpy().squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Flatten the batch of images images to build a multi-layer network.\n",
    "We have 64 images. Each image is of dimension 1 (channel) x 28 (height) x 28 (width). Thus one layer is really (64, 1, 28, 28). When this is flattened, we have 64 x 784 as we flattened teh 2D image to 1D vector.\n",
    "\n",
    "We have 256 hidden units, and 10 output units using random tensors for the weights and biases. We want 10 output to get probability for each of the numbers 0,1,2..9. \n",
    "\n",
    "In the network, we want to pass an image and get out a probability distribution over the classes that tells the likelihood of image belonging to those classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -4.0037, -10.5920, -15.2532,   1.6932,  18.8478,  13.8026,  -3.8765,\n",
      "           7.3375, -20.3124,   8.8694],\n",
      "        [ -3.7280,  -2.2973, -17.0857,   1.9139,  15.2778,   1.5556,   1.2317,\n",
      "          10.5055, -21.7185,  -0.2940],\n",
      "        [ -4.4372,  -4.7429, -16.9171,  -0.9745,  16.7698,  12.0000,   2.1957,\n",
      "          -0.5961, -18.2544,  -4.6790],\n",
      "        [ -1.9408,  -2.6723, -12.7742,   0.2631,  17.1021,  13.0295,  15.8355,\n",
      "           8.0205, -21.1886,   0.5963],\n",
      "        [ -6.8978, -10.9951,  -8.9972,   1.5503,  13.9366,   9.8109,  -2.4642,\n",
      "          -2.8042, -19.7631,  -8.9857],\n",
      "        [-11.1233,  -3.1031, -17.2671,  -1.8600,  19.3763,  14.6103,   7.9895,\n",
      "          -3.6405, -20.4525,  -7.7252],\n",
      "        [  9.2281,  -9.0607, -13.2973,   1.6014,  25.8930,   9.6311,  12.4725,\n",
      "          15.1879, -15.0954,  -4.9090],\n",
      "        [  3.3918,  -4.1994, -17.2525,  -0.3851,  16.1968,   6.7080,  -2.1579,\n",
      "           9.5091, -20.8402, -10.2223],\n",
      "        [  3.0596,  -8.4707, -21.2103,   4.5876,  17.3814,  14.8889,   2.2607,\n",
      "           1.8147, -21.4321,  -4.6967],\n",
      "        [  2.0335,   9.2317, -17.0880,  -2.0401,  25.4413,  16.4462,  11.6473,\n",
      "          20.6376, -21.5266,  -3.5433],\n",
      "        [ -3.8842, -13.5513, -13.8469,   0.5118,  23.2814,  13.6582,  -1.3529,\n",
      "           3.7091, -24.2348,   7.3393],\n",
      "        [ -1.7305,   2.9968, -19.4770,  -7.8653,  31.6378,  21.7299,  11.9237,\n",
      "           3.6942, -12.5669,  -1.3517],\n",
      "        [  2.3974,  -7.1011, -12.7711,  -7.8832,  13.1548,  17.0546,   5.2817,\n",
      "           9.5499, -25.7547,   5.2247],\n",
      "        [ -2.5672,   2.2764, -11.3256,  -0.5479,  17.1368,  10.4246,   9.9008,\n",
      "          12.9846, -21.6954,  -5.6804],\n",
      "        [  0.2244,  -2.4212, -19.5035,   0.1999,  16.3689,  15.1462,   1.2889,\n",
      "           3.0703, -19.2868,  -2.6874],\n",
      "        [  0.8161,  -3.0013, -21.9826,  10.3182,  18.6070,  -0.7448,   3.8633,\n",
      "           4.8066, -18.7319,  -9.9229],\n",
      "        [  2.0208,  -5.8864, -18.1823,  -5.2083,  17.7423,  10.2006,   7.8768,\n",
      "           8.0984, -13.4106,   4.5840],\n",
      "        [  1.4160,  -1.5553, -23.0358,  -8.3003,  23.4802,   5.7834,   2.1709,\n",
      "           7.0751, -22.4044,  -2.9811],\n",
      "        [-10.0790,   1.6426, -12.5070,   0.3114,  12.3418,   8.8428,   2.4679,\n",
      "           7.8919, -21.6450,  -6.3832],\n",
      "        [ -4.2888, -12.4295, -21.3677,  -0.6243,  15.4219,  10.9291,   8.6182,\n",
      "          -1.6631, -25.0213,  -1.8123],\n",
      "        [-11.3375, -12.5906,  -9.4487,  11.8407,  18.1491,  13.0241,   6.0658,\n",
      "           7.7114, -25.5873,  -4.1955],\n",
      "        [-10.7956,   0.8145,  -9.0901,  -3.3301,  18.8395,   8.8581,  15.3543,\n",
      "           9.8062, -22.8268,  -9.8456],\n",
      "        [ -7.8345, -14.8004, -13.8563,  -3.5253,  20.3569,   9.1866,   8.6793,\n",
      "           6.7483, -19.1173,   1.8876],\n",
      "        [-13.5388, -13.3369,  -8.7602,  -2.9337,  18.6984,   6.3232,  -0.3047,\n",
      "           3.4241, -20.4923,   1.2771],\n",
      "        [ -7.4625,  -7.3901, -10.4222,   7.3317,  13.3554,   9.8232,  -6.4561,\n",
      "           6.4694, -22.1133,   4.5352],\n",
      "        [ -5.4526,  -5.0267, -13.0664,   4.4804,  18.8606,  11.8177,  -1.3603,\n",
      "           8.2142, -20.5343,  -4.3140],\n",
      "        [ -6.2011,  -1.2086, -12.8448,   6.8134,  15.7852,  -0.1731,  -3.5695,\n",
      "           9.1183, -29.6496,  -1.2292],\n",
      "        [  6.9253,  -8.6019, -22.4201,  -3.7008,  18.2626,  18.8601,  -8.4578,\n",
      "           6.4915, -19.4967,   5.2567],\n",
      "        [ -7.8235, -10.3388,  -4.6158,   5.4708,  12.6245,  13.3175,   1.5980,\n",
      "           5.5621, -19.4689,  -8.6061],\n",
      "        [-12.5042,  -5.1490, -17.2535,   4.5336,  19.8725,  13.5722,   5.6025,\n",
      "          -6.0988, -18.0478, -17.4777],\n",
      "        [  7.2037,   5.7881, -26.5849,  -8.5205,  24.8079,   9.1954,   6.4525,\n",
      "          18.0375, -17.7050,  -8.5062],\n",
      "        [  7.3226,  12.8917, -18.3578,  -3.6870,  28.9919,   4.1893,   4.6248,\n",
      "          12.5843, -22.7834,  -3.5164],\n",
      "        [ -4.3917,   0.5205, -19.6989,  -5.6132,  11.6065,  -1.2471,   1.0731,\n",
      "          10.3925, -25.3169, -13.7343],\n",
      "        [  2.3164,   7.4817, -14.9984, -10.0890,  13.8953,  14.1996,   9.2317,\n",
      "           3.1201, -28.6517,  -8.6992],\n",
      "        [ -1.2122,  -1.5122, -21.2349,   4.3623,  27.0030,  10.3717,   8.9937,\n",
      "           3.5807, -18.7702,  -5.6040],\n",
      "        [ -4.9216, -11.8495, -17.7666,   4.3297,   9.2792,   4.5042,  -7.1844,\n",
      "          -2.8268, -21.7996,   1.9602],\n",
      "        [ -8.9580,  -6.3289, -12.8728,  -4.3052,  23.2651,   1.2062,  -5.8082,\n",
      "           4.1706, -19.9009,  -6.1051],\n",
      "        [ -8.8776,   0.8676, -10.9114,   1.2703,  17.5084,   6.3323,   2.3140,\n",
      "           7.5474, -21.2674,  -8.2914],\n",
      "        [  3.6838,   1.7460,  -9.4052,   9.6844,  14.5543,  16.5468,   8.5186,\n",
      "          -0.7342, -22.9144,   1.7692],\n",
      "        [ -3.4847,  -6.6212, -13.9489,   0.8143,  23.3495,  10.6380,  11.1834,\n",
      "          10.9100, -26.2705,  -6.0591],\n",
      "        [ -2.4095,   6.3888, -25.7545, -12.3777,  20.6556,  13.1861,  11.4274,\n",
      "          12.8888, -21.4002,  -9.4073],\n",
      "        [  0.3981,   0.3892, -15.3936,  -4.1100,  26.5963,  -0.2976,   8.5706,\n",
      "           4.7183, -22.9918,  -4.8993],\n",
      "        [ -6.6651,   3.9003, -17.7286,  -4.8950,  25.7947,  -2.5270,  -8.9731,\n",
      "           7.9154, -25.5629, -13.7050],\n",
      "        [ -4.8703,  -8.1378, -16.1117,   3.0775,  20.6643,   4.0341,  11.6508,\n",
      "           6.9832, -16.4045,  -4.1163],\n",
      "        [  4.9406,  -7.4191, -17.6541,  -2.2515,  19.0644,   7.4091,   8.3701,\n",
      "          -5.1247, -21.5312,   1.9860],\n",
      "        [ -6.8392,   0.4240, -18.5373,  -0.5115,  17.9618,   3.1512,  -4.5904,\n",
      "           7.2493, -23.2755,  -7.2083],\n",
      "        [ -2.8367,   6.1282, -21.3144,  -0.6900,  30.7231,   5.7229,  12.4487,\n",
      "          12.2448, -28.9177,  -6.3026],\n",
      "        [ -6.9001,  -8.2316, -18.4243,   6.5984,  16.3069,  13.2460,   6.0854,\n",
      "           5.0787, -20.4559,  -3.9874],\n",
      "        [ -5.0139,   3.3545, -16.7557,   2.6320,  19.9643,   9.1840,   7.8950,\n",
      "           2.0317, -20.3340,  -7.8343],\n",
      "        [ -0.1890,  -6.7656, -20.4711,  -7.1184,  23.0711,  11.1273,   1.7262,\n",
      "          14.0491, -17.1596,  -2.1277],\n",
      "        [ -0.3330,   9.9703, -12.5918,  -8.6633,  18.3108,   3.5941,   3.7545,\n",
      "           8.0482, -25.7420,  -1.4530],\n",
      "        [ -8.5144,  -2.1153,  -5.3820,   7.3370,  15.8920,  12.4526,  -0.1208,\n",
      "           3.6471, -19.2852,  -7.0152],\n",
      "        [-16.6112,  -4.3963, -20.3983,  -1.1436,  29.7244,  13.8143,   3.7847,\n",
      "           7.6589, -22.7298,  -3.3202],\n",
      "        [ -7.6326,  -1.0569, -13.2909,   7.5401,  16.2943,  11.7511,   1.9088,\n",
      "           2.2300, -27.7055,  -1.3135],\n",
      "        [ -4.1242,  -8.1960,  -9.1999,   5.0739,  20.6059,  11.4781,   8.1578,\n",
      "           5.9736, -22.1302,  -2.0518],\n",
      "        [ -4.6021,  -4.8990, -15.6592,   5.9596,  14.7183,   7.7852,   0.3677,\n",
      "          -4.8313, -27.8939,  -5.2359],\n",
      "        [  3.2451,  10.4369,  -9.2111,  -3.3023,  19.0379,   5.3791,  16.3659,\n",
      "          11.8067, -30.7758,  -3.2715],\n",
      "        [-13.5363,  -0.6179, -13.2760,   4.1011,  16.9018,  -1.6289,  12.9838,\n",
      "           0.4335, -28.0498,  -3.2251],\n",
      "        [ -0.2049,   3.0377, -11.1938,   3.1280,  13.9234,   3.5928,   8.1860,\n",
      "           0.9772, -23.3660,  -5.4392],\n",
      "        [  9.6365,   5.8079, -24.5639,  -1.3177,  15.2183,  19.5801,   6.1622,\n",
      "          -2.3770, -26.4870,  -1.5137],\n",
      "        [-10.3239,  -1.6304, -12.4424,   0.7996,  16.6617,   2.8786,   9.9806,\n",
      "           4.6097, -19.1614,   1.1791],\n",
      "        [ -5.9848,  -4.8175, -17.2956,   3.2624,  16.9307,  17.3363,   4.5019,\n",
      "           5.8558, -25.4492,  -2.3105],\n",
      "        [-17.7645, -17.6517,  -5.3152,  -0.2797,  10.5530,   3.3556,  -1.8091,\n",
      "           6.7970, -26.3611,  -1.9140],\n",
      "        [ -1.8441,  -9.3382,  -9.1827,   6.6083,  17.7288,   8.1428,  -4.1928,\n",
      "          -5.4739, -19.9655,  -5.8096]])\n"
     ]
    }
   ],
   "source": [
    "#Activiation function was previously defined (sigmoid)\n",
    "\n",
    "#Flatten the input images\n",
    "inputs = images.view(images.shape[0], -1) #-1 is shortcut to match the best shape\n",
    "\n",
    "#Create parameters\n",
    "w1 = torch.randn(784, 256)\n",
    "b1 = torch.randn(256)\n",
    "\n",
    "w2 = torch.randn(256, 10)\n",
    "b2 = torch.randn(10)\n",
    "\n",
    "h = activation(torch.mm(inputs, w1)+b1)\n",
    "out = torch.mm(h,w2)+b2\n",
    "\n",
    "print(out)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax function\n",
    "What does softmax do? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 10])\n",
      "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000,\n",
      "        1.0000])\n"
     ]
    }
   ],
   "source": [
    "def softmax(x):\n",
    "    return torch.exp(x)/torch.sum(torch.exp(x), dim=1).view(-1,1)\n",
    "\n",
    "probabilities = softmax(out)\n",
    "print(probabilities.shape)\n",
    "print(probabilities.sum(dim=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building networks with PyTorch using nn module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #input to hidden layer\n",
    "        self.hidden = nn.Linear(784,256)\n",
    "        \n",
    "        #hidden to output\n",
    "        self.output = nn.Linear(256,10)\n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.hidden(x)\n",
    "        x = self.sigmoid(x)\n",
    "        x = self.output(x)\n",
    "        x = self.softmax(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a model using the class\n",
    "model = Network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Alternaive way to build network is using funcational module\n",
    "#fairly similar to pevious way\n",
    "#Normally imported as capital F.\n",
    "# A little succinct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #input to hidden layer\n",
    "        self.hidden = nn.Linear(784,256)\n",
    "        #hidden to output\n",
    "        self.output = nn.Linear(256,10)        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Hidden layer with sigmoid activation\n",
    "        x = F.sigmoid(self.hidden(x))\n",
    "        #Output layer with Softmax actication\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (hidden): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        #input to hidden layer\n",
    "        \n",
    "        self.hidden = nn.ReLU(784,128)\n",
    "        \n",
    "        self.hidden = nn.ReLU(128,64)\n",
    "        #hidden to output\n",
    "        self.output = nn.Linear(64,10)        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        #Hidden layer with sigmoid activation\n",
    "        x = F.sigmoid(self.hidden1(x))\n",
    "        \n",
    "        x = F.sigmoid(self.hidden2(x))\n",
    "        #Output layer with Softmax actication\n",
    "        x = F.softmax(self.output(x), dim=1)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() takes from 1 to 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-3fea47629a0f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-82a44dddbd32>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;31m#input to hidden layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mReLU\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() takes from 1 to 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "model = Network()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
